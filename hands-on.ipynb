{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Multimodal Deep Learning for Recommendation (Hands-On Session)\n","\n","‚≠ê **The 2024 ACM RecSys Summer School** ‚≠ê\n","\n","*Bari (Italy), October 09, 2024*\n","\n","<div>\n","  <img src=\"https://recsys.acm.org/wp-content/uploads/2023/11/RecSysBanner_1000_180.png\" alt=\"SisInfLab\" width=\"600\">\n","  <img src=\"https://recsys.acm.org/wp-content/uploads/2020/07/Recsys-OG.png\" alt=\"recsys\" width=\"200\">\n","</div>\n","\n","üßë Instructor: [Daniele Malitesta](https://danielemalitesta.github.io/)\n","\n","üí≥ Credits: I'd like to thank [Matteo Attimonelli](mailto:matteo.attimonelli@poliba.it), [Danilo Danese](mailto:danilo.danese@poliba.it), and [Angela Di Fazio](mailto:angela.difazio@poliba.it) for their massive and amazing job on making the code well-structured and work smoothly!\n","\n","If you use this code for your experiments, please cite our recent work (on arXiv) üôè\n","\n","![GitHub Repo stars](https://img.shields.io/github/stars/sisinflab/Ducho-meets-Elliot)\n"," [![arXiv](https://img.shields.io/badge/arXiv-2409.15857-b31b1b.svg)](https://arxiv.org/abs/2409.15857)\n","\n"," <img src=\"https://github.com/sisinflab/Ducho-meets-Elliot/blob/master/framework.png?raw=true\"  width=\"700\">\n","\n","```\n","@article{DBLP:journals/corr/abs-2409-15857,\n","  author       = {Matteo Attimonelli and\n","                  Danilo Danese and\n","                  Angela Di Fazio and\n","                  Daniele Malitesta and\n","                  Claudio Pomo and\n","                  Tommaso Di Noia},\n","  title        = {Ducho meets Elliot: Large-scale Benchmarks for Multimodal Recommendation},\n","  journal      = {CoRR},\n","  volume       = {abs/2409.15857},\n","  year         = {2024}\n","}\n","```"],"metadata":{"id":"zHeOoEVlkXib"}},{"cell_type":"markdown","source":["## Clone the repository\n","\n","First, we clone the repository to exploit the Ducho + Elliot experimental environment üêë"],"metadata":{"id":"YXW5FkgZjC65"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"akRmwGzZi0vQ"},"outputs":[],"source":["!git clone --recursive https://github.com/sisinflab/Ducho-meets-Elliot.git\n","%cd Ducho-meets-Elliot/"]},{"cell_type":"markdown","source":["## Set up the working environment"],"metadata":{"id":"heMk9rBDvVqk"}},{"cell_type":"markdown","source":["Now, we setup the proper environment to run the experiments. Conveniently, Google Colab provides most of the packages we need. Thus, we only install the remaining ones üòé"],"metadata":{"id":"GAltQI_tmJXs"}},{"cell_type":"code","source":["!pip install pandas loguru alive_progress sentence_transformers\n","!pip install torch_geometric\n","!pip install torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.4.0+cu121.html\n","!python -m pip install -U prettytable"],"metadata":{"id":"m9bPQ1DRuFUV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Download and visualize the multimodal recommendation datasets"],"metadata":{"id":"ZvxiGp7LvZPs"}},{"cell_type":"markdown","source":["We're now set to download the multimodal recommendation dataset. For the sake of this lecture, we consider the popular **[Amazon Product Reviews](https://cseweb.ucsd.edu/~jmcauley/datasets.html#amazon_reviews)** dataset üõí\n","\n","Specifically, we consider the following product categories:\n","\n","| **Datasets**     | **# Users** | **# Items** | **# Interactions** | **Sparsity (%)** |\n","|------------------|-------------|-------------|--------------------|------------------|\n","| Office Products  | 4,471       | 1,703       | 20,608             | 99.73%           |\n","| Digital Music    | 5,082       | 2,338       | 30,623             | 99.74%           |\n","| Baby             | 19,100      | 6,283       | 80,931             | 99.93%           |\n","| Toys & Games     | 19,241      | 11,101      | 89,558             | 99.96%           |\n","| Beauty           | 21,752      | 11,145      | 100,834            | 99.96%           |\n","\n","For the sake of this hands-on session, we will consider only the \"Office Products\" dataset!"],"metadata":{"id":"8OPlAe10vi4E"}},{"cell_type":"code","source":["# RUN THIS CELL ONLY IF YOU HAVE TIME TO WASTE :-)\n","\n","%cd Ducho\n","!python3 ./demos/demo_office/prepare_dataset.py"],"metadata":{"id":"Qp521nsVmM1b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# OTHERWISE, HERE'S AN ALREADY-PREPARED VERSION OF THE DATASET\n","\n","import gdown\n","\n","%cd Ducho\n","!mkdir -p local/data\n","gdown.download(f'https://drive.google.com/uc?id=1DSe7osyJ5dmXRsgOkdDMPeHPHCk7eezv', 'demo_office.zip', quiet=False)\n","!mv demo_office.zip local/data/\n","!unzip local/data/demo_office.zip\n","%rm local/data/demo_office.zip"],"metadata":{"id":"wXaXlj8e_kJi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can visualize one random item from the dataset üëì"],"metadata":{"id":"A348xeBhBTFp"}},{"cell_type":"code","source":["import pandas as pd\n","import random\n","from matplotlib import pyplot as plt\n","from PIL import Image\n","from IPython.display import display, HTML\n","\n","meta = pd.read_csv('local/data/demo_office/meta.tsv', sep='\\t')\n","random_item = random.choice(meta['asin'].tolist())\n","description = meta[meta[\"asin\"]==random_item][\"description\"].values[0]\n","display(HTML(f\"ASIN: {random_item}\\n<div style='white-space: pre-wrap; width: 100%;'>{description}</div>\"))\n","img = Image.open(f'local/data/demo_office/images/{random_item}.jpg')\n","plt.imshow(img)\n","plt.axis('off')\n","plt.show()\n","\n","from prettytable import PrettyTable\n","table = PrettyTable()\n","\n","table.field_names = [\"USER\", \"Rating\"]\n","reviews = pd.read_csv('local/data/demo_office/reviews.tsv', sep='\\t', header=None)\n","current_reviews = reviews[reviews[1]==random_item]\n","\n","for idx, row in current_reviews.iterrows():\n","  table.add_row([row[0], row[2]])\n","\n","display(HTML('Clicked by:'))\n","print(table)"],"metadata":{"id":"_QkRwkpG54Vk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Check if GPU is available\n","\n","Before running any GPU-bound process, let's check if the GPU is available:"],"metadata":{"id":"LJTKjUaZHodP"}},{"cell_type":"code","source":["!nvidia-smi\n","!nvcc --version"],"metadata":{"id":"5USH4HhKHrIB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Extract multimodal features with Ducho\n","\n","![GitHub Repo stars](https://img.shields.io/github/stars/sisinflab/Ducho)\n"," [![arXiv](https://img.shields.io/badge/arXiv-2403.04503-b31b1b.svg)](https://arxiv.org/abs/2403.04503)\n","\n"," If you use Ducho for your experiments, please cite our papers üôè\n","\n","<div>\n","  <img src=\"https://github.com/sisinflab/Ducho/raw/main/docs/source/img/ducho_v2_overview.png\" alt=\"duccio\" width=\"800\">\n","</div>\n","\n","```\n","@inproceedings{DBLP:conf/www/AttimonelliDMPG24,\n","  author       = {Matteo Attimonelli and\n","                  Danilo Danese and\n","                  Daniele Malitesta and\n","                  Claudio Pomo and\n","                  Giuseppe Gassi and\n","                  Tommaso Di Noia},\n","  title        = {Ducho 2.0: Towards a More Up-to-Date Unified Framework for the Extraction\n","                  of Multimodal Features in Recommendation},\n","  booktitle    = {{WWW} (Companion Volume)},\n","  pages        = {1075--1078},\n","  publisher    = {{ACM}},\n","  year         = {2024}\n","}\n","```\n","\n","```\n","@inproceedings{DBLP:conf/mm/MalitestaGPN23,\n","  author       = {Daniele Malitesta and\n","                  Giuseppe Gassi and\n","                  Claudio Pomo and\n","                  Tommaso Di Noia},\n","  title        = {Ducho: {A} Unified Framework for the Extraction of Multimodal Features\n","                  in Recommendation},\n","  booktitle    = {{ACM} Multimedia},\n","  pages        = {9668--9671},\n","  publisher    = {{ACM}},\n","  year         = {2023}\n","}\n","```"],"metadata":{"id":"KeAhxZ6DgHxp"}},{"cell_type":"markdown","source":["Now, we are all set to extract multimodal product features through Ducho ü¶æ"],"metadata":{"id":"ynLIbsKwm_Bh"}},{"cell_type":"code","source":["# CREATE THE CONFIGURATION FILE FOR DUCHO\n","def create_config():\n","  import yaml\n","\n","  config_ducho = \"\"\"dataset_path: ./local/data/demo_office\n","gpu list: 0\n","visual:\n","    items:\n","        input_path: images\n","        output_path: visual_embeddings_32\n","        model: [\n","            { model_name: ResNet50,\n","              output_layers: avgpool,\n","              reshape: [224, 224],\n","              preprocessing: zscore,\n","              backend: torch,\n","              batch_size: 32\n","            }\n","        ]\n","\n","textual:\n","    items:\n","        input_path: meta.tsv\n","        item_column: asin\n","        text_column: description\n","        output_path: textual_embeddings_32\n","        model: [\n","          { model_name: sentence-transformers/all-mpnet-base-v2,\n","              output_layers: 1,\n","              clear_text: False,\n","              backend: sentence_transformers,\n","              batch_size: 32\n","          }\n","        ]\n","\n","visual_textual:\n","    items:\n","        input_path: {visual: images, textual: meta.tsv}\n","        item_column: asin\n","        text_column: description\n","        output_path: {visual: visual_embeddings_32, textual: textual_embeddings_32}\n","        model: [\n","          { model_name: openai/clip-vit-base-patch16,\n","              backend: transformers,\n","              output_layers: 1,\n","              batch_size: 32\n","          }\n","        ]\n","\n","  \"\"\"\n","\n","  ducho_dir = f\"demos/demo_office/config.yml\"\n","  with open(ducho_dir, 'w') as conf_file:\n","      conf_file.write(config_ducho)\n","\n","# RUN THE EXTRACTION WITH DUCHO\n","from ducho.runner.Runner import MultimodalFeatureExtractor\n","import torch\n","import os\n","import numpy as np\n","import random\n","\n","def set_seed(seed = 42):\n","    \"\"\"Set all seeds to make results reproducible (deterministic mode).\n","       When seed is None, disables deterministic mode.\n","    :param seed: an integer to your choosing\n","    \"\"\"\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":16:8\"\n","\n","\n","def main():\n","    set_seed()\n","    extractor_obj = MultimodalFeatureExtractor(config_file_path='./demos/demo_office/config.yml')\n","    extractor_obj.execute_extractions()\n","\n","\n","if __name__ == '__main__':\n","    create_config()\n","    main()"],"metadata":{"id":"2uqnTVJNeeHX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dataset splitting and features mapping in Elliot\n","\n","![GitHub Repo stars](https://img.shields.io/github/stars/sisinflab/Formal-Multimod-Rec)\n"," [![arXiv](https://img.shields.io/badge/arXiv-2309.05273-b31b1b.svg)](https://arxiv.org/abs/2309.05273)\n","\n","If everything went smoothly with the features extraction, now we can: (i) split the original dataset into train/validation/test set ‚úÇ (ii) map the multimodal item features to ids aligned with the training set üóæ\n","\n","To this end, we will use Elliot, our framework for rigorous and reproducibile recommender systems evaluation.\n","\n","If you find it useful for your research, please cite our works üôè\n","\n","\n","\n","```\n","@article{10.1145/3662738,\n","author = {Malitesta, Daniele and Cornacchia, Giandomenico and Pomo, Claudio and Merra, Felice Antonio and Di Noia, Tommaso and Di Sciascio, Eugenio},\n","title = {Formalizing Multimedia Recommendation through Multimodal Deep Learning},\n","year = {2024},\n","publisher = {Association for Computing Machinery},\n","address = {New York, NY, USA},\n","url = {https://doi.org/10.1145/3662738},\n","doi = {10.1145/3662738},\n","note = {Just Accepted},\n","journal = {ACM Trans. Recomm. Syst.},\n","month = {apr},\n","keywords = {Multimodal Deep Learning, Multimedia Recommender Systems, Benchmarking}\n","}\n","```\n","\n","\n","\n","```\n","@inproceedings{DBLP:conf/sigir/AnelliBFMMPDN21,\n","  author       = {Vito Walter Anelli and\n","                  Alejandro Bellog{\\'{\\i}}n and\n","                  Antonio Ferrara and\n","                  Daniele Malitesta and\n","                  Felice Antonio Merra and\n","                  Claudio Pomo and\n","                  Francesco Maria Donini and\n","                  Tommaso Di Noia},\n","  title        = {Elliot: {A} Comprehensive and Rigorous Framework for Reproducible\n","                  Recommender Systems Evaluation},\n","  booktitle    = {{SIGIR}},\n","  pages        = {2405--2414},\n","  publisher    = {{ACM}},\n","  year         = {2021}\n","}\n","```\n","\n","\n","\n","**Dataset splitting**\n","\n","* Train = 80% dataset\n","\n","* Test = 20% dataset\n","\n","* Valid = 10% train set"],"metadata":{"id":"BjZn0kK1jVr3"}},{"cell_type":"code","source":["%cd ..\n","\n","%mv ./Ducho/local/data/demo_office/visual_embeddings_32 ./data/office\n","%mv ./Ducho/local/data/demo_office/textual_embeddings_32 ./data/office\n","\n","%cp ./Ducho/local/data/demo_office/reviews.tsv ./data/office\n","\n","split_config = '''\n","experiment:\n","  backend: pytorch\n","  data_config:\n","    strategy: dataset\n","    dataset_path: ../data/office/reviews.tsv\n","  splitting:\n","    save_on_disk: True\n","    save_folder: ../data/office_splits/\n","    test_splitting:\n","      strategy: random_subsampling\n","      test_ratio: 0.2\n","    validation_splitting:\n","      strategy: random_subsampling\n","      test_ratio: 0.1\n","  dataset: office\n","  top_k: 20\n","  evaluation:\n","    cutoffs: [ 10, 20 ]\n","    simple_metrics: [ Recall, nDCG ]\n","  gpu: 0\n","  external_models_path: ../external/models/__init__.py\n","  models:\n","    MostPop:\n","      meta:\n","        verbose: True\n","        save_recs: False\n","'''\n","\n","split_dir = f\"./config_files/split_office.yml\"\n","with open(split_dir, 'w') as conf_file:\n","    conf_file.write(split_config)\n","\n","# SPLIT INTO TRAIN/VAL/TEST\n","%env CUBLAS_WORKSPACE_CONFIG=:16:8\n","!python3 run_split.py --dataset office\n","\n","%cd ./data/office\n","\n","%env CUBLAS_WORKSPACE_CONFIG=:16:8\n","\n","# MAP ITEMS TO NUMERICAL IDS\n","train = pd.read_csv('train.tsv', sep='\\t', header=None)\n","val = pd.read_csv('val.tsv', sep='\\t', header=None)\n","test = pd.read_csv('test.tsv', sep='\\t', header=None)\n","\n","df = pd.concat([train, val, test], axis=0)\n","\n","users = df[0].unique()\n","items = df[1].unique()\n","\n","users_map = {u: idx for idx, u in enumerate(users)}\n","items_map = {i: idx for idx, i in enumerate(items)}\n","\n","train[0] = train[0].map(users_map)\n","train[1] = train[1].map(items_map)\n","\n","val[0] = val[0].map(users_map)\n","val[1] = val[1].map(items_map)\n","\n","test[0] = test[0].map(users_map)\n","test[1] = test[1].map(items_map)\n","\n","train.to_csv('train_indexed.tsv', sep='\\t', index=False, header=None)\n","val.to_csv('val_indexed.tsv', sep='\\t', index=False, header=None)\n","test.to_csv('test_indexed.tsv', sep='\\t', index=False, header=None)\n","\n","visual_embeddings_folder = f'visual_embeddings_32/torch/ResNet50/avgpool'\n","textual_embeddings_folder = f'textual_embeddings_32/sentence_transformers/sentence-transformers/all-mpnet-base-v2/1'\n","\n","visual_embeddings_folder_indexed = f'visual_embeddings_indexed_32/torch/ResNet50/avgpool'\n","textual_embeddings_folder_indexed = f'textual_embeddings_indexed_32/sentence_transformers/sentence-transformers/all-mpnet-base-v2/1'\n","\n","if not os.path.exists(visual_embeddings_folder_indexed):\n","    os.makedirs(visual_embeddings_folder_indexed)\n","\n","if not os.path.exists(textual_embeddings_folder_indexed):\n","    os.makedirs(textual_embeddings_folder_indexed)\n","\n","for key, value in items_map.items():\n","    np.save(f'{visual_embeddings_folder_indexed}/{value}.npy', np.load(f'{visual_embeddings_folder}/{key}.npy'))\n","    np.save(f'{textual_embeddings_folder_indexed}/{value}.npy', np.load(f'{textual_embeddings_folder}/{key}.npy'))\n","\n","\n","visual_embeddings_folder = f'visual_embeddings_32/transformers/openai/clip-vit-base-patch16/1'\n","textual_embeddings_folder = f'textual_embeddings_32/transformers/openai/clip-vit-base-patch16/1'\n","\n","visual_embeddings_folder_indexed = f'visual_embeddings_indexed_32/transformers/openai/clip-vit-base-patch16/1'\n","textual_embeddings_folder_indexed = f'textual_embeddings_indexed_32/transformers/openai/clip-vit-base-patch16/1'\n","\n","if not os.path.exists(visual_embeddings_folder_indexed):\n","    os.makedirs(visual_embeddings_folder_indexed)\n","\n","if not os.path.exists(textual_embeddings_folder_indexed):\n","    os.makedirs(textual_embeddings_folder_indexed)\n","\n","for key, value in items_map.items():\n","    np.save(f'{visual_embeddings_folder_indexed}/{value}.npy', np.load(f'{visual_embeddings_folder}/{key}.npy'))\n","    np.save(f'{textual_embeddings_folder_indexed}/{value}.npy', np.load(f'{textual_embeddings_folder}/{key}.npy'))\n","\n","\n","%cd ../../"],"metadata":{"id":"UZKTpmUmi5pu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The downloaded multimodal dataset has the following structure:\n","\n","```\n","‚îú‚îÄ‚îÄ office\n","‚îÇ   ‚îú‚îÄ‚îÄ visual_embeddings_indexed_32\n","|       ‚îú‚îÄ‚îÄ torch\n","|          ‚îú‚îÄ‚îÄ ResNet50\n","|             ‚îú‚îÄ‚îÄ avgpool\n","‚îÇ                ‚îú‚îÄ‚îÄ 0.npy\n","‚îÇ                ‚îú‚îÄ‚îÄ 1.npy\n","‚îÇ                ‚îú‚îÄ‚îÄ ...\n","|       ‚îú‚îÄ‚îÄ transformers\n","|          ‚îú‚îÄ‚îÄ openai\n","|             ‚îú‚îÄ‚îÄ clip-vit-base-patch16\n","|                ‚îú‚îÄ‚îÄ 1\n","‚îÇ                   ‚îú‚îÄ‚îÄ 0.npy\n","‚îÇ                   ‚îú‚îÄ‚îÄ 1.npy\n","‚îÇ                   ‚îú‚îÄ‚îÄ ...\n","|\n","‚îÇ   ‚îú‚îÄ‚îÄ textual_embeddings_indexed_32\n","|       ‚îú‚îÄ‚îÄ sentence_transformers\n","|          ‚îú‚îÄ‚îÄ sentence-transformers\n","|             ‚îú‚îÄ‚îÄ all-mpnet-base-v2\n","|                ‚îú‚îÄ‚îÄ 1\n","‚îÇ                   ‚îú‚îÄ‚îÄ 0.npy\n","‚îÇ                   ‚îú‚îÄ‚îÄ 1.npy\n","‚îÇ                   ‚îú‚îÄ‚îÄ ...\n","|       ‚îú‚îÄ‚îÄ transformers\n","|          ‚îú‚îÄ‚îÄ openai\n","|             ‚îú‚îÄ‚îÄ clip-vit-base-patch16\n","|                ‚îú‚îÄ‚îÄ 1\n","‚îÇ                   ‚îú‚îÄ‚îÄ 0.npy\n","‚îÇ                   ‚îú‚îÄ‚îÄ 1.npy\n","‚îÇ                   ‚îú‚îÄ‚îÄ ...\n","‚îÇ   ‚îú‚îÄ‚îÄ train_indexed.tsv\n","‚îÇ   ‚îú‚îÄ‚îÄ val_indexed.tsv\n","‚îÇ   ‚îú‚îÄ‚îÄ test_indexed.tsv\n","```\n","\n"],"metadata":{"id":"dqdHyO8inLnw"}},{"cell_type":"markdown","source":["## Configure and run the experiments\n","Let's set the hyper-parameters for the model to be trained and tested. We will focus on VBPR in a modified version which adopts multimodal features. We train and evaluate it on Amazon Office."],"metadata":{"id":"z02UP4r9cVEd"}},{"cell_type":"markdown","source":["### First multimodal features configuration\n","\n","We start with the configuration ResNet50 (visual) + SentenceBert (textual), the most common one from the literature."],"metadata":{"id":"61kNdayAckeH"}},{"cell_type":"code","source":["import yaml\n","\n","config_filename = 'hands-on_resnet50_sentencebert'\n","elliot_config_1 = {\n","  'experiment': {\n","    'backend': 'pytorch',\n","    'data_config': {\n","      'strategy': 'fixed',\n","      'train_path': '../data/{0}/train_indexed.tsv',\n","      'validation_path': '../data/{0}/val_indexed.tsv',\n","      'test_path': '../data/{0}/test_indexed.tsv',\n","      'side_information': [\n","        {\n","            'dataloader': 'VisualAttribute',\n","            'visual_features': '../data/{0}/visual_embeddings_indexed_32/torch/ResNet50/avgpool'\n","        },\n","        {\n","            'dataloader': 'TextualAttribute',\n","            'textual_features': '../data/{0}/textual_embeddings_indexed_32/sentence_transformers/sentence-transformers/all-mpnet-base-v2/1'\n","        }\n","      ]\n","    },\n","    'dataset': 'office',\n","    'top_k': 20,\n","    'evaluation': {\n","      'cutoffs': [20],\n","      'simple_metrics': ['Recall', 'Precision', 'nDCG', 'HR']\n","    },\n","    'gpu': 0,\n","    'external_models_path': '../external/models/__init__.py',\n","    'models': {\n","      'external.VBPR': {\n","        'meta': {\n","          'hyper_opt_alg': 'grid',\n","          'verbose': True,\n","          'save_weights': False,\n","          'save_recs': False,\n","          'validation_rate': 10,\n","          'validation_metric': 'Recall@20',\n","          'restore': False\n","        },\n","        'epochs': 200,\n","        'batch_size': 1024,\n","        'factors': 64,\n","        'lr': 0.005,\n","        'l_w': 1e-5,\n","        'n_layers': 1,\n","        'comb_mod': 'concat',\n","        'modalities': \"('visual','textual')\",\n","        'loaders': \"('VisualAttribute','TextualAttribute')\",\n","        'seed': 123\n","      }\n","    }\n","  }\n","}\n","\n","with open(f'config_files/{config_filename}.yml', 'w') as file:\n","    documents = yaml.dump(elliot_config_1, file)"],"metadata":{"id":"YxBi4sqkccQW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Run Elliot\n","\n","Now we are all set to run an experiment with VBPR on Amazon Office with the first multimodal configuration."],"metadata":{"id":"fWN7Ns1re0Vf"}},{"cell_type":"code","source":["from elliot.run import run_experiment\n","\n","run_experiment(f\"config_files/hands-on_resnet50_sentencebert.yml\")"],"metadata":{"id":"Q8igy2uce6ad"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Second multimodal features configuration\n","\n","Second, we prepare and run the second configuration for the multimodal feature extractors. In this case, we use CLIP, a popular multimodal model in the deep learning literature, but largely overlooked in the recommendation community.\n","\n"],"metadata":{"id":"DM_lprmkfH9H"}},{"cell_type":"code","source":["config_filename = 'hands-on_clip'\n","elliot_config_2 = {\n","  'experiment': {\n","    'backend': 'pytorch',\n","    'data_config': {\n","      'strategy': 'fixed',\n","      'train_path': '../data/{0}/train_indexed.tsv',\n","      'validation_path': '../data/{0}/val_indexed.tsv',\n","      'test_path': '../data/{0}/test_indexed.tsv',\n","      'side_information': [\n","        {\n","            'dataloader': 'VisualAttribute',\n","            'visual_features': '../data/{0}/visual_embeddings_indexed_32/transformers/openai/clip-vit-base-patch16/1'\n","        },\n","        {\n","            'dataloader': 'TextualAttribute',\n","            'textual_features': '../data/{0}/textual_embeddings_indexed_32/transformers/openai/clip-vit-base-patch16/1'\n","        }\n","      ]\n","    },\n","    'dataset': 'office',\n","    'top_k': 20,\n","    'evaluation': {\n","      'cutoffs': [20],\n","      'simple_metrics': ['Recall', 'Precision', 'nDCG', 'HR']\n","    },\n","    'gpu': 0,\n","    'external_models_path': '../external/models/__init__.py',\n","    'models': {\n","      'external.VBPR': {\n","        'meta': {\n","          'hyper_opt_alg': 'grid',\n","          'verbose': True,\n","          'save_weights': False,\n","          'save_recs': False,\n","          'validation_rate': 10,\n","          'validation_metric': 'Recall@20',\n","          'restore': False\n","        },\n","        'epochs': 200,\n","        'batch_size': 1024,\n","        'factors': 64,\n","        'lr': 0.005,\n","        'l_w': 1e-5,\n","        'n_layers': 1,\n","        'comb_mod': 'concat',\n","        'modalities': \"('visual','textual')\",\n","        'loaders': \"('VisualAttribute','TextualAttribute')\",\n","        'seed': 123\n","      }\n","    }\n","  }\n","}\n","\n","with open(f'config_files/{config_filename}.yml', 'w') as file:\n","    documents = yaml.dump(elliot_config_2, file)"],"metadata":{"id":"RydIwe78ffpW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Run Elliot\n","\n","Now we are all set to run an experiment with VBPR on Amazon Office with the second multimodal configuration."],"metadata":{"id":"eeP3QSryfm1B"}},{"cell_type":"code","source":["from elliot.run import run_experiment\n","\n","run_experiment(f\"config_files/hands-on_clip.yml\")"],"metadata":{"id":"P_osRKwlfqHo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Final comments\n","\n","We see that with different multimodal feature extractors, results are not the same. Indeed, with CLIP, we find (in most cases) improved recommendation performance than the usual ones obtained with ResNet50 + SentenceBert. That happens even without having explored a wide hyper-parameter space.\n","\n","\n","\n","\n","\n","```\n","# First configuration (ResNet50 + SentenceBert)\n","\n","{20: {'Recall': 0.07367239361200444, 'Precision': 0.008286736747931112, 'nDCG': 0.034714471891791276, 'HR': 0.1433683739655558}}\n","```\n","\n","\n","\n","```\n","# Second configuration (CLIP)\n","\n","{20: {'Recall': 0.07386561970547656, 'Precision': 0.008443301274882577, 'nDCG': 0.03508305857211589, 'HR': 0.14538134645493178}}\n","```\n","\n"],"metadata":{"id":"PIvb0QhQgP-_"}}]}